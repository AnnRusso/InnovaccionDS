{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: azure-cognitiveservices-vision-face in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.6.0)\n",
      "Requirement already satisfied: azure-common~=1.1 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-cognitiveservices-vision-face) (1.1.28)\n",
      "Requirement already satisfied: msrest>=0.5.0 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-cognitiveservices-vision-face) (0.7.1)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.3.1)\n",
      "Requirement already satisfied: azure-core>=1.24.0 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (2022.6.15)\n",
      "Requirement already satisfied: requests~=2.16 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (2.28.0)\n",
      "Requirement already satisfied: isodate>=0.6.0 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from msrest>=0.5.0->azure-cognitiveservices-vision-face) (0.6.1)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.0.1 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from azure-core>=1.24.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (4.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests~=2.16->msrest>=0.5.0->azure-cognitiveservices-vision-face) (2.0.12)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\anahi\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.5.0->msrest>=0.5.0->azure-cognitiveservices-vision-face) (3.2.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\Anahi\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install azure-cognitiveservices-vision-face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to use cognitive services at https://ejemplo-caras.cognitiveservices.azure.com/ using key 7792715916944474bc677dda15495c43\n"
     ]
    }
   ],
   "source": [
    "cog_key = '7792715916944474bc677dda15495c43'\n",
    "cog_endpoint = 'https://ejemplo-caras.cognitiveservices.azure.com/'\n",
    "print('Ready to use cognitive services at {} using key {}'.format(cog_endpoint, cog_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.cognitiveservices.vision.face import FaceClient\n",
    "from msrest.authentication import CognitiveServicesCredentials\n",
    "import faces\n",
    "import os\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear un cliente de detección facial.\n",
    "face_client = FaceClient(cog_endpoint, CognitiveServicesCredentials(cog_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Abrir una imagen\n",
    "image_path ='../LabCV/Data/vision/store_cam2.jpg'\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detectar caras\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "\n",
    "# Mostrar las caras (código en python_code/faces.py)\n",
    "faces.show_faces(image_path, detected_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ejecute la siguiente celda para ver los ID de la cara de algunos clientes.\n",
    "# Abrir una imagen\n",
    "\n",
    "image_path ='../LabCV/Data/vision/store_cam3.jpg'\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detectar caras\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "\n",
    "# Mostrar las caras (código en python_code/faces.py)\n",
    "faces.show_faces(image_path, detected_faces, show_id=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analizar los atributos faciales\n",
    "Face puede ir mucho más allá del simple reconocimiento facial. \n",
    " También puede analizar las características y expresiones de las caras \n",
    " para indicar la edad y el estado emocional. \n",
    " Ejecute el siguiente código para analizar los atributos faciales de un cliente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir una imagen\n",
    "image_path ='../LabCV/data/vision/store_cam1.jpg'\n",
    "image_stream = open(image_path, \"rb\")\n",
    "\n",
    "# Detectar caras y determinados atributos faciales\n",
    "attributes = ['age', 'emotion']\n",
    "detected_faces = face_client.face.detect_with_stream(image=image_stream, return_face_attributes=attributes)\n",
    "\n",
    "# Mostrar las caras y los atributos (código en python_code/faces.py)\n",
    "faces.show_face_attributes(image_path, detected_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Búsqueda de caras similares\n",
    "Los ID de caras creados para cada cara detectada se usan para identificar cada una de ellas. Puede usar estos ID para comparar una cara detectada con otras detectadas anteriormente y encontrar caras con características similares.\n",
    "\n",
    "Por ejemplo, ejecute la siguiente celda para comparar el cliente de una imagen con los clientes de otra y encontrar una cara que coincida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener el identificador de la primera cara de la imagen 1\n",
    "image_1_path ='../LabCV/data/vision/store_cam3.jpg'\n",
    "image_1_stream = open(image_1_path, \"rb\")\n",
    "image_1_faces = face_client.face.detect_with_stream(image=image_1_stream)\n",
    "face_1 = image_1_faces[0]\n",
    "\n",
    "# Obtener los identificadores de caras en una segunda imagen\n",
    "image_2_path ='../LabCV/data/vision/store_cam2.jpg'\n",
    "image_2_stream = open(image_2_path, \"rb\")\n",
    "image_2_faces = face_client.face.detect_with_stream(image=image_2_stream)\n",
    "image_2_face_ids = list(map(lambda face: face.face_id, image_2_faces))\n",
    "\n",
    "# Encontrar las caras de la imagen 2 que sean similares a la de la imagen 1\n",
    "similar_faces = face_client.face.find_similar(face_id=face_1.face_id, face_ids=image_2_face_ids)\n",
    "\n",
    "# Mostrar la cara de la imagen 1 y las caras similares de la imagen 2 (código en python_code/face.py)\n",
    "faces.show_similar_faces(image_1_path, face_1, image_2_path, image_2_faces, similar_faces)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconocimiento de caras\n",
    "Hasta ahora, hemos visto que Face puede detectar caras y características faciales e identificar dos caras similares entre sí. Podemos ir más allá si implementamos una solución de reconocimiento facial en la que se entrene a Face para reconocer la cara de una persona en concreto. Esto puede ser útil en diferentes casos, como para etiquetar fotografías de amigos automáticamente en redes sociales o usar el reconocimiento facial como un sistema biométrico de verificación de identidad.\n",
    "\n",
    "Para ver cómo funciona, supongamos que Northwind Traders quiere usar el reconocimiento facial para garantizar que solo los empleados autorizados del departamento de TI pueda acceder a los sistemas seguros.\n",
    "\n",
    "Lo primero que haremos es crear un grupo de personas que represente a los empleados autorizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_id = 'employee_group_id'\n",
    "try:\n",
    "    # Elimine el grupo si ya existe\n",
    "    face_client.person_group.delete(group_id)\n",
    "except Exception as ex:\n",
    "    print(ex.message)\n",
    "finally:\n",
    "    face_client.person_group.create(group_id, 'employees')\n",
    "    print ('Group created!')\n",
    "    group_id = 'employee_group_id'\n",
    "try:\n",
    "    # Elimine el grupo si ya existe\n",
    "    face_client.person_group.delete(group_id)\n",
    "except Exception as ex:\n",
    "    print(ex.message)\n",
    "finally:\n",
    "    face_client.person_group.create(group_id, 'employees')\n",
    "    print ('Group created!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez creado el grupo de personas, podemos agregar una persona para cada empleado que queramos incluir en el grupo y, después, agregar varias fotografías de cada persona para que Face pueda analizar las características faciales de cada persona. Lo ideal es que las imágenes muestren a la misma persona con diferentes poses y gestos.\n",
    "\n",
    "Agregaremos un solo empleado llamado Wendell y tres fotografías suyas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "# Agregar una persona (Wendell) al grupo\n",
    "wendell = face_client.person_group_person.create(group_id, 'Wendell')\n",
    "\n",
    "# Obtener una foto de wendell\n",
    "folder ='../data/face/wendell'\n",
    "wendell_pics = os.listdir(folder)\n",
    "\n",
    "# Registrar las fotos\n",
    "i = 0\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "for pic in wendell_pics:\n",
    "    # Agregue cada foto a la persona en el grupo de personas\n",
    "    img_path = os.path.join(folder, pic)\n",
    "    img_stream = open(img_path, \"rb\")\n",
    "    face_client.person_group_person.add_face_from_stream(group_id, wendell.person_id, img_stream)\n",
    "\n",
    "    # Muestre cada imagen\n",
    "    img = Image.open(img_path)\n",
    "    i +=1\n",
    "    a=fig.add_subplot(1,len(wendell_pics), i)\n",
    "    a.axis('off')\n",
    "    imgplot = plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez agregadas la persona y las fotografías, podemos entrenar a Face para que reconozca a Wendell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_client.person_group.train(group_id)\n",
    "print('Trained!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez entrenado el modelo, podemos usarlo para identificar caras reconocidas en una imagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener los identificadores de caras en una segunda imagen\n",
    "image_path ='../data/face/employees.jpg'\n",
    "image_stream = open(image_path, \"rb\")\n",
    "image_faces = face_client.face.detect_with_stream(image=image_stream)\n",
    "image_face_ids = list(map(lambda face: face.face_id, image_faces))\n",
    "\n",
    "# Obtener los nombres de caras reconocidas\n",
    "face_names = {}\n",
    "recognized_faces = face_client.face.identify(image_face_ids, group_id)\n",
    "for face in recognized_faces:\n",
    "    person_name = face_client.person_group_person.get(group_id, face.candidates[0].person_id).name\n",
    "    face_names[face.face_id] = person_name\n",
    "\n",
    "# Mostrar las caras reconocidas\n",
    "faces.show_recognized_faces(image_path, image_faces, face_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f61b940798d3a001e94d2cd98f16974579f86cc44d33c5ecf2c6b6cc1038e06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
